{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "l=[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def f(x):\n",
    "    d = dict()\n",
    "    d[0] = x\n",
    "    \n",
    "f(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'assignment.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpy_compile\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m py_compile\u001b[39m.\u001b[39;49mcompile(\u001b[39m\"\u001b[39;49m\u001b[39massignment.py\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/py_compile.py:142\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(file, cfile, dfile, doraise, optimize, invalidation_mode, quiet)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(cfile))\n\u001b[1;32m    141\u001b[0m loader \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mmachinery\u001b[39m.\u001b[39mSourceFileLoader(\u001b[39m'\u001b[39m\u001b[39m<py_compile>\u001b[39m\u001b[39m'\u001b[39m, file)\n\u001b[0;32m--> 142\u001b[0m source_bytes \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mget_data(file)\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     code \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39msource_to_code(source_bytes, dfile \u001b[39mor\u001b[39;00m file,\n\u001b[1;32m    145\u001b[0m                                  _optimize\u001b[39m=\u001b[39moptimize)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1073\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assignment.py'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[2, 3]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "ll = deepcopy(l)\n",
    "ll.remove(1)\n",
    "print(deepcopy(l).remove(1))\n",
    "print(ll)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transporter_env import TransportEnv, MAT, OneDynamicTransporter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# env = MAT(max_capacity=10)#size=12, transporters_hubs=(27, 116), horizon=128)\n",
    "# env = OneDynamicTransporter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2626b1ff8644add853f9f3a2378f473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:682: TqdmExperimentalWarning: rich is experimental/alpha\n",
      "  self.pbar = tqdm(total=self.locals[\"total_timesteps\"] - self.model.num_timesteps)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(2e5), progress_bar=True)\n",
    "# Save the agent\n",
    "model.save(\"ppo\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained agent\n",
    "# NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
    "# to compare the system on which the model was trained vs the current one\n",
    "# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
    "model = PPO.load(\"ppo\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "l = np.array([1,0, 1])\n",
    "k = ''.join([str(x) for x in l])\n",
    "hash(k)\n",
    "key = lambda l : ''.join([str(x) for x in l])\n",
    "key_to_np = lambda k : np.array(list(k), dtype=int)\n",
    "print(key_to_np(key(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 5]),)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros(10)\n",
    "a[1] = 1\n",
    "a[5] = 1\n",
    "np.where(a == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Enjoy trained agent\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = []\n",
    "# env.reset()\n",
    "# env.render()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = 1\n",
    "#     _, r, d, *_ = env.step(action)\n",
    "#     done = d\n",
    "#     rewards.append(r)\n",
    "# env.render()\n",
    "# t1_r = [r for r in rewards]\n",
    "# # t2_r = [r[1] for r in rewards]\n",
    "# print('transporter 1 total profit', sum(t1_r))\n",
    "# # print('transporter 2 total profit', sum(t2_r))\n",
    "\n",
    "def make_env():\n",
    "    import gymnasium as gym\n",
    "    e = gym.make('CartPole-v1')\n",
    "    e.t = 16\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from rlberry.agents import Agent\n",
    "from rlberry.manager import AgentManager, plot_writer_data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(torch.backends.mps.is_built())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlberry.agents import Agent\n",
    "\n",
    "class MyAgent(Agent):\n",
    "  name = \"MyAgent\"\n",
    "  def __init__(self, env, **kwargs):\n",
    "    \"\"\"\n",
    "    The base class (Agent) initializes:\n",
    "      self.env : instance of the environment used for training (in fit() method)\n",
    "      self.eval_env : instance of the environment used for evaluation (in eval() method)\n",
    "      self.rng : random number generator (https://numpy.org/doc/stable/reference/random/generator.html)\n",
    "      self.writer : use self.writer.add_scalar(tag, value, global_step) to log training data\n",
    "    \n",
    "    For reproducibility, use ONLY self.rng if you need random numbers in you agent!\n",
    "    To be able to visualize plots with AgentManager, log data using self.writer (see below)\n",
    "    \"\"\"\n",
    "    Agent.__init__(self, env, **kwargs)\n",
    "    # self.param1 = param1\n",
    "    # self.param2 = param2\n",
    "    self.total_steps = 0\n",
    "    self.total_episodes = 0\n",
    "\n",
    "  def select_action(self, state, evaluation=False):\n",
    "    \"\"\"\n",
    "    If evaluation=True, run evaluation policy (e.g., greedy with respect to Q)\n",
    "    If evaluation=False, run exploration policy (e.g., epsilon greedy)\n",
    "    \"\"\"\n",
    "    return self.env.action_space.sample()  # random action for this example\n",
    "\n",
    "  def fit(self, budget):\n",
    "    \"\"\"budget = number of timesteps to train your agent\"\"\"\n",
    "    state, _ = self.env.reset()\n",
    "    episode_reward = 0.0\n",
    "    for tt in range(budget):\n",
    "      self.total_steps += 1\n",
    "      action = self.select_action(state, evaluation=False)\n",
    "      next_state, reward, done, *_ = self.env.step(action)\n",
    "      episode_reward += reward\n",
    "      # print(reward)\n",
    "\n",
    "      # Log data\n",
    "      self.writer.add_scalar('rewards', reward, global_step=self.total_steps)\n",
    "\n",
    "      state = next_state \n",
    "      if done:\n",
    "        self.total_episodes += 1\n",
    "        # Log episode data\n",
    "        self.writer.add_scalar('episode_rewards', episode_reward, global_step=self.total_steps)\n",
    "        self.writer.add_scalar('episode', self.total_episodes, global_step=self.total_steps)\n",
    "\n",
    "        state, _ = self.env.reset()\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "  \n",
    "  def eval(self, **kwargs):\n",
    "    \"\"\"\n",
    "    Here, you can run Monte-Carlo policy evaluation \n",
    "    with self.eval_env and return the result.\n",
    "    Returning zero for this example.\n",
    "    \"\"\"\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: (Re)defining the following DefaultWriter parameters in AgentManager: ['maxlen', 'log_interval'] \u001b[0m\n",
      "\u001b[38;21m[INFO] 13:41: Running AgentManager fit() for MyAgent with n_fit = 2 and max_workers = None. \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name      tag  value global_step  dw_time_elapsed\n",
      "0    MyAgent  rewards    1.0           1         0.000215\n",
      "1    MyAgent  rewards    1.0           2         0.000231\n",
      "2    MyAgent  rewards    1.0           3         0.000241\n",
      "3    MyAgent  rewards    1.0           4         0.000249\n",
      "4    MyAgent  rewards    1.0           5         0.000257\n",
      "..       ...      ...    ...         ...              ...\n",
      "105  MyAgent  episode    1.0          16         0.000346\n",
      "106  MyAgent  episode    2.0          26         0.000468\n",
      "107  MyAgent  episode    3.0          38         0.000594\n",
      "108  MyAgent  episode    4.0          85         0.000960\n",
      "109  MyAgent  episode    5.0          99         0.001081\n",
      "\n",
      "[110 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m[INFO] 13:41: ... trained! \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n",
      "\u001b[38;5;226m[WARNING] 13:41: [Agent] Not possible to reseed environment. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Initialize and train a single instance of MyAgent\n",
    "#\n",
    "env = gym.make('CartPole-v1')\n",
    "my_agent = MyAgent(\n",
    "    env=env,#(env, {}),       # tuple (constructor, kwargs)\n",
    "    # param1=10,               # extra params your agent might need\n",
    "    # param2=15\n",
    ")\n",
    "# train the agent for 100 timesteps\n",
    "my_agent.fit(100)\n",
    "# pandas DataFrame containing data stored with my_agent.writer.add_scalar(tag, value, global_step)\n",
    "print(my_agent.writer.data)\n",
    "\n",
    "#\n",
    "# Run several instances of MyAgent in parallel and plot the results\n",
    "#\n",
    "manager_kwargs = dict(\n",
    "    agent_class=MyAgent,\n",
    "    train_env=(make_env, dict()),\n",
    "    eval_env=(make_env, dict()),\n",
    "    fit_budget=100,                    # Number of total timesteps\n",
    "    n_fit=2,                           # Number of agent instances to fit\n",
    "    parallelization='thread',          # Use 'thread' in the notebook!\n",
    "    seed=456,                          # Seed\n",
    "    default_writer_kwargs=dict(maxlen=100,log_interval=10),\n",
    ")\n",
    "my_agent_manager = AgentManager(\n",
    "    # init_kwargs=dict(param1=10, param2=20),\n",
    "    agent_name='MyAgent',\n",
    "    **manager_kwargs\n",
    ")\n",
    "my_agent_manager.fit()   # Train 'n_fit' instances in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: videos: File exists\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "%mkdir videos\n",
    "# Initialize display and import function to show videos\n",
    "\n",
    "def render_policy(env, agent):\n",
    "  env = deepcopy(env)\n",
    "  # env = RecordVideo(env, './videos')\n",
    "  for episode in range(1):\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    env.render()\n",
    "    while not done:\n",
    "        action = agent.select_action(state, evaluation=True)\n",
    "        state, reward, done, *_ = env.step(action)\n",
    "    env.render()\n",
    "    env.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:212: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Plot the results\n",
    "example_managers = []\n",
    "example_managers.append(my_agent_manager)    # You could add more managers here, for other agents/parameters\n",
    "_ = plot_writer_data(example_managers, tag='rewards', title='Instantaneous Rewards')\n",
    "# _ = plot_writer_data(example_managers, tag='episode_rewards', xtag='episode', title='Episode Rewards')\n",
    "# _ = plot_writer_data(example_managers, tag='episode_rewards', xtag='episode', title='Cumulative Episode Rewards', preprocess_func=np.cumsum)\n",
    "\n",
    "# Render the policy of one of the trained agents\n",
    "agent_instance = my_agent_manager.get_agent_instances()[0]\n",
    "render_policy(agent_instance.eval_env, agent_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "  def __init__(self, capacity, rng):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    capacity : int\n",
    "      Maximum number of transitions\n",
    "    rng : \n",
    "      instance of numpy's default_rng\n",
    "    \"\"\"\n",
    "    self.capacity = capacity\n",
    "    self.rng = rng  # random number generator\n",
    "    self.memory = []\n",
    "    self.position = 0\n",
    "\n",
    "  def push(self, sample):\n",
    "    \"\"\"Saves a transition.\"\"\"\n",
    "    if len(self.memory) < self.capacity:\n",
    "      self.memory.append(None)\n",
    "    self.memory[self.position] = sample\n",
    "    self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    indices = self.rng.choice(len(self.memory), size=batch_size)\n",
    "    samples = [self.memory[idx] for idx in indices]\n",
    "    return map(np.asarray, zip(*samples))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "  def __init__(self, obs_size, n_actions):\n",
    "    super(QNet, self).__init__()\n",
    "    \n",
    "    #print(f'obs size = {obs_size}')\n",
    "    self.lin1 = nn.Linear(obs_size, 64)\n",
    "    self.lin2 = nn.Linear(64, 64)\n",
    "    self.lin3 = nn.Linear(64, n_actions)\n",
    "    #self.relu = nn.ReLU().to(device)\n",
    "\n",
    "  def forward(self, state):\n",
    "\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "      state = torch.tensor(state).float().to(device)\n",
    "    #else :\n",
    "    #  state = state.clone().detach().float().to(device)\n",
    "    \n",
    "    Q = F.relu(self.lin1(state))\n",
    "    Q = F.relu(self.lin2(Q))\n",
    "    return self.lin3(Q)\n",
    "\n",
    "class DuelingQNet(nn.Module):\n",
    "  def __init__(self, obs_size, n_actions):\n",
    "    super(DuelingQNet, self).__init__()\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "    self.lin = nn.Linear(obs_size, 64)\n",
    "\n",
    "    # value function net\n",
    "    self.val_lin1 = nn.Linear(64, 64)\n",
    "    self.val_lin2 = nn.Linear(64, 1)\n",
    "\n",
    "    # advantage function net\n",
    "    self.adv_lin1 = nn.Linear(64, 64)\n",
    "    self.adv_lin2 = nn.Linear(64, n_actions)\n",
    "\n",
    "  def forward(self, state):\n",
    "\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "      state = torch.tensor(state).float().to(device)\n",
    "    #else :\n",
    "      #state = state.clone().detach().float().to(device)\n",
    "\n",
    "    y = self.lin(state)\n",
    "    y = self.relu(y)\n",
    "\n",
    "    # value function\n",
    "    v = self.val_lin1(y)\n",
    "    v = self.relu(v)\n",
    "    v = self.val_lin2(v)\n",
    "\n",
    "    # advantage function\n",
    "    a = self.adv_lin1(y)\n",
    "    a = self.relu(a)\n",
    "    a = self.adv_lin2(a)\n",
    "\n",
    "    return v + (a - torch.mean(a))\n",
    "  \n",
    "class LSTMQNet(nn.Module):\n",
    "  def __init__(self, obs_size, n_actions):\n",
    "    super(DuelingQNet, self).__init__()\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "    self.lstm = nn.LSTM(obs_size, 64)\n",
    "\n",
    "    self.lin1 = nn.Linear(64, 64)\n",
    "    self.lin2 = nn.Linear(64, n_actions)\n",
    "\n",
    "  def forward(self, state):\n",
    "\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "      state = torch.tensor(state).float().to(device)\n",
    "    #else :\n",
    "      #state = state.clone().detach().float().to(device)\n",
    "\n",
    "    y = self.lstm(state)\n",
    "    y = self.relu(y)\n",
    "\n",
    "    # value function\n",
    "    v = self.lin1(y)\n",
    "    v = self.relu(v)\n",
    "    return self.lin2(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DQN_TRAINING_TIMESTEPS = 2_000  # number of timesteps for training. You might change this!\n",
    "\n",
    "DQN_PARAMS = dict(\n",
    "    dueling_dqn=False,         # set to true to use dueling DQN\n",
    "    double_dqn=False,          # set to true to use double DQN\n",
    "    gamma=0.99,\n",
    "    batch_size=128,            # batch size (in number of transitions)\n",
    "    eval_every=250,            # evaluate every ... steps\n",
    "    buffer_capacity=30000,     # capacity of the replay buffer\n",
    "    update_target_every=250,   # update target net every ... steps\n",
    "    epsilon_start=1.0,         # initial value of epsilon\n",
    "    epsilon_min=0.05,          # minimum value of epsilon\n",
    "    decrease_epsilon=5000,     # parameter to decrease epsilon\n",
    "    learning_rate=5e-4,       # learning rate\n",
    ")\n",
    "DUELING_DQN_PARAMS = deepcopy(DQN_PARAMS)           # dueling DQN\n",
    "DOUBLE_DQN_PARAMS = deepcopy(DQN_PARAMS)            # double DQN\n",
    "DOUBLE_DUELING_DQN_PARAMS = deepcopy(DQN_PARAMS)    # double & dueling DQN\n",
    "# LSTM_DQN_PARAMS = deepcopy(DQN_PARAMS)\n",
    "\n",
    "DUELING_DQN_PARAMS.update(dict(dueling_dqn=True))\n",
    "DOUBLE_DQN_PARAMS.update(dict(double_dqn=True))\n",
    "DOUBLE_DUELING_DQN_PARAMS.update(dueling_dqn=True, double_dqn=True)\n",
    "# LSTM_DQN_PARAMS.update(dict(rnn = False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "  name = 'DQN'\n",
    "  def __init__(\n",
    "      self,\n",
    "      env,\n",
    "      dueling_dqn: bool,               # Set to true for dueling DQN\n",
    "      double_dqn: bool,                # Set to true for double DQN\n",
    "      gamma: float = 0.99,\n",
    "      batch_size: int = 256,\n",
    "      eval_every: int = 250,\n",
    "      buffer_capacity: int = 30000,\n",
    "      update_target_every: int = 500,\n",
    "      epsilon_start: float = 1.0,\n",
    "      epsilon_min: float = 0.05,\n",
    "      decrease_epsilon: int = 200,\n",
    "      learning_rate: float = 0.001,\n",
    "      **kwargs):\n",
    "    Agent.__init__(self, env, **kwargs)\n",
    "    env = self.env\n",
    "    self.dueling_dqn = dueling_dqn\n",
    "    self.double_dqn = double_dqn\n",
    "    self.gamma = gamma\n",
    "    self.batch_size = batch_size\n",
    "    self.eval_every = eval_every\n",
    "    self.update_target_every = update_target_every\n",
    "    self.epsilon_start = epsilon_start\n",
    "    self.epsilon_min = epsilon_min\n",
    "    self.decrease_epsilon = decrease_epsilon\n",
    "    self.total_timesteps = 0\n",
    "    self.total_episodes = 0\n",
    "    self.total_updates = 0\n",
    "\n",
    "    # initialize epsilon\n",
    "    self.epsilon = epsilon_start\n",
    "\n",
    "    # initialize replay buffer\n",
    "    self.replay_buffer = ReplayBuffer(buffer_capacity, self.rng)\n",
    "\n",
    "    # select network class\n",
    "    if self.dueling_dqn:\n",
    "      net_class = DuelingQNet\n",
    "    else:\n",
    "      net_class = QNet\n",
    "      \n",
    "    # if self.rnn:\n",
    "      # net_class = LSTMQNet\n",
    "\n",
    "    # update name according to params\n",
    "    if self.dueling_dqn:\n",
    "      self.name = 'Dueling' + self.name\n",
    "    if self.double_dqn:\n",
    "      self.name = 'Double' + self.name\n",
    "\n",
    "    # create network and target network\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    self.q_net = net_class(obs_size, n_actions).to(device)\n",
    "    self.target_net = net_class(obs_size, n_actions).to(device)\n",
    "    self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "    self.target_net.eval()\n",
    "    \n",
    "    # objective and optimizer\n",
    "    self.optimizer = optim.Adam(\n",
    "        params=self.q_net.parameters(), lr=learning_rate)\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "\n",
    "  def select_action(self, state, evaluation=False):\n",
    "    \"\"\"\n",
    "    If evaluation=False, get action according to exploration policy.\n",
    "    Otherwise, get action according to the evaluation policy.\n",
    "    \"\"\"\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # TODO: implement action selection strategy\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    #print(f'random action = {self.env.action_space.sample()}')\n",
    "    \n",
    "    if self.rng.uniform() < self.epsilon and (not evaluation):\n",
    "      action = self.env.action_space.sample()   # this happens with probability epsilon\n",
    "    else:\n",
    "      with torch.no_grad():\n",
    "        action = torch.argmax(self.q_net(state)).item()\n",
    "\n",
    "    return action # replace by the action you computed\n",
    "  \n",
    "  #@torch.compile\n",
    "  def _update(self, batch_state, batch_action, batch_next_state, batch_reward):\n",
    "    # values = Q(s_t, a_t), for t in batch\n",
    "        values = self.q_net(batch_state).gather(1, batch_action.long())\n",
    "\n",
    "        if not self.double_dqn:\n",
    "          # !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "          # TO DO: compute DQN targets\n",
    "          # !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "          with torch.no_grad():\n",
    "            #next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "            next_state_values = self.target_net(batch_next_state).max(1)[0].unsqueeze(1).to(device)\n",
    "            targets = (next_state_values * self.gamma) + batch_reward\n",
    "      \n",
    "        else:\n",
    "          # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "          # TO DO: compute Double DQN targets\n",
    "          # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "          with torch.no_grad():\n",
    "            next_state_values = self.target_net(batch_next_state).gather(1, torch.argmax(self.q_net(batch_next_state), dim=1).unsqueeze(1)).to(device)\n",
    "            targets = (next_state_values * self.gamma) + batch_reward\n",
    "\n",
    "\n",
    "        loss = self.loss_fn(values, targets)#.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #for param in self.q_net.parameters():\n",
    "        #  param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "  def fit(self, budget):\n",
    "    \"\"\"\n",
    "    budget : number of training timesteps\n",
    "    \"\"\"\n",
    "    state, _ = self.env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    for tt in range(budget):\n",
    "      self.total_timesteps += 1\n",
    "      # if self.env.t < 15:\n",
    "      #   action = 0\n",
    "      # else:\n",
    "      action = self.select_action(state, evaluation=False)\n",
    "      next_state, reward, done, *_ = self.env.step(action)\n",
    "      episode_reward += reward\n",
    "      # if not done :\n",
    "      #   self.replay_buffer.push((state, next_state, action, reward, done))\n",
    "      # else :\n",
    "      #   self.replay_buffer.push((state, next_state, action, -reward, done))\n",
    "      self.replay_buffer.push((state, next_state, action, float(not done)*reward, done))\n",
    "        \n",
    "      if len(self.replay_buffer) > self.batch_size:\n",
    "        #\n",
    "        # Update model\n",
    "        #\n",
    "        self.total_updates += 1\n",
    "\n",
    "        # get batch\n",
    "        (batch_state, batch_next_state,\n",
    "         batch_action, batch_reward,\n",
    "         batch_done) = self.replay_buffer.sample(self.batch_size)\n",
    "        # convert to torch tensors\n",
    "        batch_state = torch.FloatTensor(batch_state).to(device)\n",
    "        batch_next_state = torch.FloatTensor(batch_next_state).to(device)\n",
    "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)\n",
    "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n",
    "\n",
    "        # decrease epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= (self.epsilon_start \n",
    "                             - self.epsilon_min) / self.decrease_epsilon\n",
    "\n",
    "        #\n",
    "        #  TO DO: compute loss and update networks\n",
    "        #\n",
    "        loss = self._update(batch_state, batch_action, batch_next_state, batch_reward)\n",
    "        \n",
    "\n",
    "        self.writer.add_scalar('loss', loss.item(), self.total_timesteps)\n",
    "\n",
    "      # evaluate agent\n",
    "      if self.total_timesteps % self.eval_every == 0:\n",
    "          mean_rewards = self.eval(n_sim=2)\n",
    "          self.writer.add_scalar(\n",
    "              'eval_rewards', mean_rewards, self.total_timesteps)\n",
    "      \n",
    "      # update target network\n",
    "      if self.total_updates % self.update_target_every == 0:\n",
    "          self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "          self.target_net.eval()\n",
    "\n",
    "      # check end of episode\n",
    "      state = next_state\n",
    "      if done:\n",
    "        state, _ = self.env.reset()\n",
    "        self.total_episodes += 1\n",
    "        self.writer.add_scalar(\n",
    "                'episode_rewards', episode_reward, self.total_timesteps)\n",
    "        self.writer.add_scalar(\n",
    "                'episode', self.total_episodes, self.total_timesteps)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "  def eval(self, n_sim=1, **kwargs):\n",
    "    rewards = np.zeros(n_sim)\n",
    "    eval_env = self.eval_env     # evaluation environment\n",
    "    # Loop over number of simulations\n",
    "    for sim in range(n_sim):\n",
    "      state, _ = eval_env.reset()\n",
    "      done = False\n",
    "      while not done:\n",
    "        action = self.select_action(state, evaluation=True)\n",
    "        #print(f'action = {type(action)}')\n",
    "        next_state, reward, done, *_ = eval_env.step(action)\n",
    "        # update sum of rewards\n",
    "        rewards[sim] += reward\n",
    "        state = next_state\n",
    "    return rewards.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training one instance of DQN\n",
    "# dqn_agent = DQNAgent(\n",
    "#     env=(get_env, dict()),  # we can send (constructor, kwargs) as an env\n",
    "#     **DQN_PARAMS\n",
    "# )\n",
    "# dqn_agent.fit(DQN_TRAINING_TIMESTEPS)\n",
    "\n",
    "#\n",
    "# Training several instances using AgentManager\n",
    "#\n",
    "    \n",
    "manager_kwargs = dict(\n",
    "    agent_class=DQNAgent,\n",
    "    train_env=(make_env, dict()),\n",
    "    eval_env=(make_env, dict()),\n",
    "    fit_budget=DQN_TRAINING_TIMESTEPS,\n",
    "    n_fit=3,                   # NOTE: You may increase this parameter (number of agents to train)\n",
    "    parallelization='process',\n",
    "    seed=456,\n",
    "    default_writer_kwargs=dict(maxlen=None,log_interval=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m[WARNING] 13:41: (Re)defining the following DefaultWriter parameters in AgentManager: ['maxlen', 'log_interval'] \u001b[0m\n",
      "\u001b[38;21m[INFO] 13:41: Running AgentManager fit() for DQN with n_fit = 3 and max_workers = None. \u001b[0m\n",
      "Process SpawnProcess-3:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-4:\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'DQNAgent' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-2:\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'DQNAgent' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'DQNAgent' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# DQN\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dqn_manager \u001b[39m=\u001b[39m AgentManager(\n\u001b[1;32m      3\u001b[0m     init_kwargs\u001b[39m=\u001b[39mDOUBLE_DQN_PARAMS,\n\u001b[1;32m      4\u001b[0m     agent_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDQN\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmanager_kwargs\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m dqn_manager\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/rlberry/manager/agent_manager.py:744\u001b[0m, in \u001b[0;36mAgentManager.fit\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    742\u001b[0m         workers_output \u001b[39m=\u001b[39m []\n\u001b[1;32m    743\u001b[0m         \u001b[39mfor\u001b[39;00m future \u001b[39min\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mas_completed(futures):\n\u001b[0;32m--> 744\u001b[0m             workers_output\u001b[39m.\u001b[39mappend(future\u001b[39m.\u001b[39;49mresult())\n\u001b[1;32m    745\u001b[0m         executor\u001b[39m.\u001b[39mshutdown()\n\u001b[1;32m    747\u001b[0m workers_output\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mid)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.11/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "# DQN\n",
    "dqn_manager = AgentManager(\n",
    "    init_kwargs=DOUBLE_DQN_PARAMS,\n",
    "    agent_name='DQN',\n",
    "    **manager_kwargs\n",
    ")\n",
    "dqn_manager.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'episode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'episode'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Plot the results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m _ \u001b[39m=\u001b[39m plot_writer_data(dqn_manager, tag\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrewards\u001b[39m\u001b[39m'\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInstantaneous Rewards\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m _ \u001b[39m=\u001b[39m plot_writer_data(example_managers, tag\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mepisode_rewards\u001b[39;49m\u001b[39m'\u001b[39;49m, xtag\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mepisode\u001b[39;49m\u001b[39m'\u001b[39;49m, title\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mEpisode Rewards\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m _ \u001b[39m=\u001b[39m plot_writer_data(example_managers, tag\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepisode_rewards\u001b[39m\u001b[39m'\u001b[39m, xtag\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepisode\u001b[39m\u001b[39m'\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCumulative Episode Rewards\u001b[39m\u001b[39m'\u001b[39m, preprocess_func\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mcumsum)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Render the policy of one of the trained agents\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/rlberry/manager/evaluation.py:470\u001b[0m, in \u001b[0;36mplot_writer_data\u001b[0;34m(data_source, tag, xtag, id_agent, ax, show, preprocess_func, title, savefig_fname, sns_kwargs, smooth_weight, plot_raw_curves)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m xtag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     xtag \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mglobal_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 470\u001b[0m \u001b[39mif\u001b[39;00m data[xtag]\u001b[39m.\u001b[39mnotnull()\u001b[39m.\u001b[39msum() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    471\u001b[0m     xx \u001b[39m=\u001b[39m xtag\n\u001b[1;32m    472\u001b[0m     \u001b[39mif\u001b[39;00m data[\u001b[39m\"\u001b[39m\u001b[39mglobal_step\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misna()\u001b[39m.\u001b[39msum() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'episode'"
     ]
    }
   ],
   "source": [
    "# Plot the results\n",
    "_ = plot_writer_data(dqn_manager, tag='rewards', title='Instantaneous Rewards')\n",
    "# _ = plot_writer_data(dqn_manager, tag='episode_rewards', xtag='episode', title='Episode Rewards')\n",
    "# _ = plot_writer_data(dqn_manager, tag='episode_rewards', xtag='episode', title='Cumulative Episode Rewards', preprocess_func=np.cumsum)\n",
    "\n",
    "# Render the policy of one of the trained agents\n",
    "agent_instance = dqn_manager.get_agent_instances()[0]\n",
    "render_policy(agent_instance.eval_env, agent_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 12:28:20,264\tINFO worker.py:1382 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://\n",
      "\u001b[2m\u001b[1m\u001b[33m(autoscaler +7m54s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 12:28:36,134\tINFO trainable.py:172 -- Trainable.setup took 15.860 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-03-11 12:28:36,135\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "2023-03-11 12:28:36,143\tERROR actor_manager.py:496 -- Ray error, taking actor 1 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=14413, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x300f57b50>)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 174, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 914, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 323, in run\n",
      "    outputs = self.step()\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 379, in step\n",
      "    self._base_env.send_actions(actions_to_send)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n",
      "    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 236, in step\n",
      "    return self.env.step(actions)\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 197, in step\n",
      "    return self._get_state(), self._get_rewards(actions), done, trunc, self._get_info()\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 150, in _get_rewards\n",
      "    node = int(self.obs[\"0\"][2])\n",
      "KeyError: '0'\n",
      "2023-03-11 12:28:36,144\tERROR actor_manager.py:496 -- Ray error, taking actor 2 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=14414, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x15fc57b20>)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 174, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 914, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 323, in run\n",
      "    outputs = self.step()\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 379, in step\n",
      "    self._base_env.send_actions(actions_to_send)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n",
      "    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 236, in step\n",
      "    return self.env.step(actions)\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 197, in step\n",
      "    return self._get_state(), self._get_rewards(actions), done, trunc, self._get_info()\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 150, in _get_rewards\n",
      "    node = int(self.obs[\"0\"][2])\n",
      "KeyError: '0'\n",
      "2023-03-11 12:28:36,144\tERROR actor_manager.py:496 -- Ray error, taking actor 3 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=14415, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x300553b20>)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 174, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 914, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 323, in run\n",
      "    outputs = self.step()\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 379, in step\n",
      "    self._base_env.send_actions(actions_to_send)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n",
      "    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 236, in step\n",
      "    return self.env.step(actions)\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 197, in step\n",
      "    return self._get_state(), self._get_rewards(actions), done, trunc, self._get_info()\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 150, in _get_rewards\n",
      "    node = int(self.obs[\"0\"][2])\n",
      "KeyError: '0'\n",
      "2023-03-11 12:28:36,145\tERROR actor_manager.py:496 -- Ray error, taking actor 4 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=14416, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x300b57b50>)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 174, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 914, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 323, in run\n",
      "    outputs = self.step()\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 379, in step\n",
      "    self._base_env.send_actions(actions_to_send)\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n",
      "    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 236, in step\n",
      "    return self.env.step(actions)\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 197, in step\n",
      "    return self._get_state(), self._get_rewards(actions), done, trunc, self._get_info()\n",
      "  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 150, in _get_rewards\n",
      "    node = int(self.obs[\"0\"][2])\n",
      "KeyError: '0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14415)\u001b[0m obs is : {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14415)\u001b[0m ids is : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14415)\u001b[0m /opt/homebrew/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14415)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14413)\u001b[0m /opt/homebrew/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14413)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14413)\u001b[0m 2023-03-11 12:28:36,068\tWARNING env.py:296 -- Your MultiAgentEnv <MAT instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14414)\u001b[0m /opt/homebrew/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14414)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14416)\u001b[0m /opt/homebrew/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14416)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "ename": "RayTaskError(KeyError)",
     "evalue": "\u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=14413, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x300f57b50>)\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n    raise e\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 174, in apply\n    return func(self, *args, **kwargs)\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n    lambda w: w.sample(), local_worker=False, healthy_only=True\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 914, in sample\n    batches = [self.input_reader.next()]\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n    batches = [self.get_data()]\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n    item = next(self._env_runner)\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 323, in run\n    outputs = self.step()\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 379, in step\n    self._base_env.send_actions(actions_to_send)\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n    raise e\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 236, in step\n    return self.env.step(actions)\n  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 197, in step\n    return self._get_state(), self._get_rewards(actions), done, trunc, self._get_info()\n  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 150, in _get_rewards\n    node = int(self.obs[\"0\"][2])\nKeyError: '0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(KeyError)\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m\n\u001b[1;32m     39\u001b[0m config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mmulti_agent(\n\u001b[1;32m     40\u001b[0m     policies \u001b[39m=\u001b[39m {\n\u001b[1;32m     41\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m : PolicySpec(config\u001b[39m=\u001b[39msac\u001b[39m.\u001b[39mSACConfig\u001b[39m.\u001b[39moverrides()), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     policy_mapping_fn \u001b[39m=\u001b[39m (\u001b[39mlambda\u001b[39;00m agent_id, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: agent_id),\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m algo \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mbuild()  \n\u001b[0;32m---> 48\u001b[0m algo\u001b[39m.\u001b[39;49mtrain() \n\u001b[1;32m     49\u001b[0m \u001b[39m# Other settings we might adjust:\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m# config[\"num_workers\"] = 10                       # Use > 1 for using more CPU cores, including over a cluster\u001b[39;00m\n\u001b[1;32m     51\u001b[0m                                                 \u001b[39m# I.e., for each minibatch of data, do this many passes over it to train. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m#     with open('ma_ppo3/rewards.json', 'w') as outfile:\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m#         json.dump(episode_json, outfile)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:368\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    367\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 368\u001b[0m     \u001b[39mraise\u001b[39;00m skipped \u001b[39mfrom\u001b[39;00m \u001b[39mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    370\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mstep() needs to return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m \u001b[39m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:365\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    364\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 365\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    366\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    367\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:782\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     (\n\u001b[1;32m    775\u001b[0m         results,\n\u001b[1;32m    776\u001b[0m         train_iter_ctx,\n\u001b[1;32m    777\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    778\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[1;32m    784\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:2713\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2711\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2712\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m-> 2713\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[1;32m   2714\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2715\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn.py:405\u001b[0m, in \u001b[0;36mDQN.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m store_weight, sample_and_train_weight \u001b[39m=\u001b[39m calculate_rr_weights(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig)\n\u001b[1;32m    403\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(store_weight):\n\u001b[1;32m    404\u001b[0m     \u001b[39m# Sample (MultiAgentBatch) from workers.\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m     new_sample_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    406\u001b[0m         worker_set\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers, concat\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    409\u001b[0m     \u001b[39m# Update counters\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_sample_batch\u001b[39m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py:85\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     82\u001b[0m     sample_batches \u001b[39m=\u001b[39m [worker_set\u001b[39m.\u001b[39mlocal_worker()\u001b[39m.\u001b[39msample()]\n\u001b[1;32m     83\u001b[0m \u001b[39m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     sample_batches \u001b[39m=\u001b[39m worker_set\u001b[39m.\u001b[39;49mforeach_worker(\n\u001b[1;32m     86\u001b[0m         \u001b[39mlambda\u001b[39;49;00m w: w\u001b[39m.\u001b[39;49msample(), local_worker\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, healthy_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m worker_set\u001b[39m.\u001b[39mnum_healthy_remote_workers() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     89\u001b[0m         \u001b[39m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[39m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:701\u001b[0m, in \u001b[0;36mWorkerSet.foreach_worker\u001b[0;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    690\u001b[0m     local_result \u001b[39m=\u001b[39m [func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_worker())]\n\u001b[1;32m    692\u001b[0m remote_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__worker_manager\u001b[39m.\u001b[39mforeach_actor(\n\u001b[1;32m    693\u001b[0m     func,\n\u001b[1;32m    694\u001b[0m     healthy_only\u001b[39m=\u001b[39mhealthy_only,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m     mark_healthy\u001b[39m=\u001b[39mmark_healthy,\n\u001b[1;32m    699\u001b[0m )\n\u001b[0;32m--> 701\u001b[0m handle_remote_call_result_errors(remote_results, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ignore_worker_failures)\n\u001b[1;32m    703\u001b[0m \u001b[39m# With application errors handled, return good results.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m remote_results \u001b[39m=\u001b[39m [r\u001b[39m.\u001b[39mget() \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m remote_results\u001b[39m.\u001b[39mignore_errors()]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:74\u001b[0m, in \u001b[0;36mhandle_remote_call_result_errors\u001b[0;34m(results, ignore_worker_failures)\u001b[0m\n\u001b[1;32m     72\u001b[0m     logger\u001b[39m.\u001b[39mexception(r\u001b[39m.\u001b[39mget())\n\u001b[1;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m r\u001b[39m.\u001b[39mget()\n",
      "\u001b[0;31mRayTaskError(KeyError)\u001b[0m: \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=14413, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x300f57b50>)\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n    raise e\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 174, in apply\n    return func(self, *args, **kwargs)\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n    lambda w: w.sample(), local_worker=False, healthy_only=True\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 914, in sample\n    batches = [self.input_reader.next()]\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n    batches = [self.get_data()]\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n    item = next(self._env_runner)\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 323, in run\n    outputs = self.step()\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 379, in step\n    self._base_env.send_actions(actions_to_send)\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 656, in send_actions\n    raise e\n  File \"/opt/homebrew/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py\", line 645, in send_actions\n    obs, rewards, terminateds, truncateds, infos = env.step(agent_dict)\n  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 236, in step\n    return self.env.step(actions)\n  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 197, in step\n    return self._get_state(), self._get_rewards(actions), done, trunc, self._get_info()\n  File \"/Users/faridounet/PhD/TransportersDilemma/transporter_env.py\", line 150, in _get_rewards\n    node = int(self.obs[\"0\"][2])\nKeyError: '0'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14414)\u001b[0m obs is : {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14414)\u001b[0m ids is : []\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14413)\u001b[0m obs is : {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14413)\u001b[0m ids is : []\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14416)\u001b[0m obs is : {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14416)\u001b[0m ids is : []\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.sac import sac\n",
    "from transporter_env import TransportEnv, MAT, OneDynamicTransporter\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "checkpoint_root = \"./ma_sac\"\n",
    "# Where checkpoints are written:\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)\n",
    "\n",
    "# Where some data will be written and used by Tensorboard below:\n",
    "ray_results = f'{os.getenv(\"HOME\")}/ray_results/'\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)\n",
    "\n",
    "info = ray.init(ignore_reinit_error=True)\n",
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "# register_env(\"MAT\", MAT)\n",
    "\n",
    "#Configs\n",
    "N_ITER = 1000                                     # Number of training runs.\n",
    "\n",
    "config = sac.SACConfig()             # PPO's default configuration. See the next code cell.\n",
    "config = config.environment(env=MAT, disable_env_checking=False)\n",
    "# config = config.training(\n",
    "#     num_sgd_iter = 10, # Number of SGD (stochastic gradient descent) iterations per training minibatch.\n",
    "#     #vf_clip_param = 100,\n",
    "# ) \n",
    "config = config.framework('torch')\n",
    "\n",
    "config = config.rollouts(num_rollout_workers=4) \n",
    "config = config.multi_agent(\n",
    "    policies = {\n",
    "        \"0\" : PolicySpec(config=sac.SACConfig.overrides()), \n",
    "        \"1\" : PolicySpec(config=sac.SACConfig.overrides())\n",
    "    },\n",
    "    policy_mapping_fn = (lambda agent_id, *args, **kwargs: agent_id),\n",
    ")\n",
    "\n",
    "algo = config.build()  \n",
    "algo.train() \n",
    "# Other settings we might adjust:\n",
    "# config[\"num_workers\"] = 10                       # Use > 1 for using more CPU cores, including over a cluster\n",
    "                                                # I.e., for each minibatch of data, do this many passes over it to train. \n",
    "# config[\"model\"][\"fcnet_hiddens\"] = [100, 50]    #\n",
    "\n",
    "# config.environment(disable_env_checking=True)\n",
    "\n",
    "# from ray.rllib.policy.policy import PolicySpec, Policy\n",
    "    \n",
    "# def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "#     return agent_id\n",
    "\n",
    "    \n",
    "\n",
    "# config[\"multiagent\"] = {\n",
    "#     \"policies\" : {\n",
    "#         \"0\" : PolicySpec(\n",
    "#             action_space=env.action_space\n",
    "#         ),\n",
    "#         \"1\" : PolicySpec(\n",
    "#             action_space=env.action_space\n",
    "#         )\n",
    "#     },\n",
    "#     \"policy_mapping_fn\":\n",
    "#             policy_mapping_fn,\n",
    "#     \"policies_to_train\": [\"0\", \"1\"],\n",
    "# }\n",
    "\n",
    "# #Trainer\n",
    "# agent = ppo.PPOTrainer(config, env=SELECT_ENV)\n",
    "\n",
    "# results = []\n",
    "# episode_data = []\n",
    "# episode_json = []\n",
    "\n",
    "# for n in range(N_ITER):\n",
    "#     result = agent.train()\n",
    "#     results.append(result)\n",
    "    \n",
    "#     episode = {'n': n, \n",
    "#                'episode_reward_min': result['episode_reward_min'], \n",
    "#                'episode_reward_mean': result['episode_reward_mean'], \n",
    "#                'episode_reward_max': result['episode_reward_max'],  \n",
    "#                'episode_len_mean': result['episode_len_mean']}\n",
    "    \n",
    "#     episode_data.append(episode)\n",
    "#     episode_json.append(json.dumps(episode))\n",
    "#     file_name = agent.save(checkpoint_root)\n",
    "    \n",
    "#     print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')\n",
    "\n",
    "#     with open('ma_ppo3/rewards.json', 'w') as outfile:\n",
    "#         json.dump(episode_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 14:36:47,210\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://\n",
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'transporter_env.OneDynamicTransporter'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'is_atari': None, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x2a4a6f2e0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 10, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x2a4a66d40>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/xmdymgfj6t79xkdq239jvsmm0000gn/T/ipykernel_28195/1633673784.py:20: DeprecationWarning: Accessing values through ctx[\"webui_url\"] is deprecated. Use ctx.address_info[\"webui_url\"] instead.\n",
      "  print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import ppo\n",
    "from transporter_env import TransportEnv, MAT, OneDynamicTransporter\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "checkpoint_root = \"./ma_ppo\"\n",
    "# Where checkpoints are written:\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)\n",
    "\n",
    "# Where some data will be written and used by Tensorboard below:\n",
    "ray_results = f'{os.getenv(\"HOME\")}/ray_results/'\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)\n",
    "\n",
    "info = ray.init(ignore_reinit_error=True)\n",
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from transporter_env import OneDynamicTransporter\n",
    "\n",
    "# register_env(\"OneDynamicTransporter\", OneDynamicTransporter)\n",
    "\n",
    "#Configs\n",
    "N_ITER = 1000                                     # Number of training runs.\n",
    "\n",
    "config = ppo.PPOConfig()             # PPO's default configuration. See the next code cell.\n",
    "config = config.environment(env=OneDynamicTransporter, disable_env_checking=True)\n",
    "config = config.training(\n",
    "    num_sgd_iter = 10, # Number of SGD (stochastic gradient descent) iterations per training minibatch.\n",
    "    #vf_clip_param = 100,\n",
    ") \n",
    "config = config.framework('torch')\n",
    "\n",
    "config = config.rollouts(num_rollout_workers=4) \n",
    "# config = config.resources(num_gpus=1) \n",
    "# config = config.training(fcnet_hiddens =[128, 64] )\n",
    "# config = config.multi_agent(\n",
    "#     policies = {\"0\", \"1\"},\n",
    "#     policy_mapping_fn = (lambda agent_id, *args, **kwargs: agent_id),\n",
    "# )\n",
    "# dic = config.to_dict()\n",
    "# q_config = dic['q_model_config']\n",
    "# q_config['fcnet_hiddens'] = [128, 64]\n",
    "# policy_config = dic['policy_model_config']\n",
    "# policy_config['fcnet_hiddens'] = [128, 64]\n",
    "# config = config.training(\n",
    "#     q_model_config = q_config,\n",
    "#     policy_model_config = policy_config,\n",
    "# )\n",
    "\n",
    "print(config.to_dict())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 14:36:49,500\tWARNING unified.py:54 -- Could not instantiate TBXLogger: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
      " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates.\n",
      "2023-03-11 14:36:49,501\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2023-03-11 14:36:51,559\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28254)\u001b[0m /opt/homebrew/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28254)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28253)\u001b[0m /opt/homebrew/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28253)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28255)\u001b[0m /opt/homebrew/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28255)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28252)\u001b[0m /opt/homebrew/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28252)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: Min/Mean/Max reward:   0.0000/  0.0000/  0.0000. Checkpoint saved to ./ma_ppo/checkpoint_000001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m episode_json \u001b[39m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_ITER):\n\u001b[0;32m---> 38\u001b[0m     result \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     39\u001b[0m     results\u001b[39m.\u001b[39mappend(result)\n\u001b[1;32m     41\u001b[0m     episode \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m: n, \n\u001b[1;32m     42\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mepisode_reward_min\u001b[39m\u001b[39m'\u001b[39m: result[\u001b[39m'\u001b[39m\u001b[39mepisode_reward_min\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     43\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mepisode_reward_mean\u001b[39m\u001b[39m'\u001b[39m: result[\u001b[39m'\u001b[39m\u001b[39mepisode_reward_mean\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     44\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m'\u001b[39m: result[\u001b[39m'\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m'\u001b[39m],  \n\u001b[1;32m     45\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mepisode_len_mean\u001b[39m\u001b[39m'\u001b[39m: result[\u001b[39m'\u001b[39m\u001b[39mepisode_len_mean\u001b[39m\u001b[39m'\u001b[39m]}\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:365\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    364\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 365\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    366\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    367\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:782\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     (\n\u001b[1;32m    775\u001b[0m         results,\n\u001b[1;32m    776\u001b[0m         train_iter_ctx,\n\u001b[1;32m    777\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    778\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[1;32m    784\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:2713\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2711\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2712\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m-> 2713\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[1;32m   2714\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2715\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo.py:358\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m     train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    354\u001b[0m         worker_set\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers,\n\u001b[1;32m    355\u001b[0m         max_agent_steps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    359\u001b[0m         worker_set\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers, max_env_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mtrain_batch_size\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    361\u001b[0m train_batch \u001b[39m=\u001b[39m train_batch\u001b[39m.\u001b[39mas_multi_agent()\n\u001b[1;32m    362\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_batch\u001b[39m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py:85\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     82\u001b[0m     sample_batches \u001b[39m=\u001b[39m [worker_set\u001b[39m.\u001b[39mlocal_worker()\u001b[39m.\u001b[39msample()]\n\u001b[1;32m     83\u001b[0m \u001b[39m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     sample_batches \u001b[39m=\u001b[39m worker_set\u001b[39m.\u001b[39;49mforeach_worker(\n\u001b[1;32m     86\u001b[0m         \u001b[39mlambda\u001b[39;49;00m w: w\u001b[39m.\u001b[39;49msample(), local_worker\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, healthy_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m worker_set\u001b[39m.\u001b[39mnum_healthy_remote_workers() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     89\u001b[0m         \u001b[39m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[39m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:692\u001b[0m, in \u001b[0;36mWorkerSet.foreach_worker\u001b[0;34m(self, func, local_worker, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[39mif\u001b[39;00m local_worker \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_worker() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m     local_result \u001b[39m=\u001b[39m [func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_worker())]\n\u001b[0;32m--> 692\u001b[0m remote_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__worker_manager\u001b[39m.\u001b[39;49mforeach_actor(\n\u001b[1;32m    693\u001b[0m     func,\n\u001b[1;32m    694\u001b[0m     healthy_only\u001b[39m=\u001b[39;49mhealthy_only,\n\u001b[1;32m    695\u001b[0m     remote_actor_ids\u001b[39m=\u001b[39;49mremote_worker_ids,\n\u001b[1;32m    696\u001b[0m     timeout_seconds\u001b[39m=\u001b[39;49mtimeout_seconds,\n\u001b[1;32m    697\u001b[0m     return_obj_refs\u001b[39m=\u001b[39;49mreturn_obj_refs,\n\u001b[1;32m    698\u001b[0m     mark_healthy\u001b[39m=\u001b[39;49mmark_healthy,\n\u001b[1;32m    699\u001b[0m )\n\u001b[1;32m    701\u001b[0m handle_remote_call_result_errors(remote_results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ignore_worker_failures)\n\u001b[1;32m    703\u001b[0m \u001b[39m# With application errors handled, return good results.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py:588\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    579\u001b[0m     func, remote_actor_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_func_and_remote_actor_id_by_state(\n\u001b[1;32m    580\u001b[0m         func, remote_actor_ids\n\u001b[1;32m    581\u001b[0m     )\n\u001b[1;32m    583\u001b[0m remote_calls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_actors(\n\u001b[1;32m    584\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m    585\u001b[0m     remote_actor_ids\u001b[39m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    586\u001b[0m )\n\u001b[0;32m--> 588\u001b[0m _, remote_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__fetch_result(\n\u001b[1;32m    589\u001b[0m     remote_actor_ids\u001b[39m=\u001b[39;49mremote_actor_ids,\n\u001b[1;32m    590\u001b[0m     remote_calls\u001b[39m=\u001b[39;49mremote_calls,\n\u001b[1;32m    591\u001b[0m     timeout_seconds\u001b[39m=\u001b[39;49mtimeout_seconds,\n\u001b[1;32m    592\u001b[0m     return_obj_refs\u001b[39m=\u001b[39;49mreturn_obj_refs,\n\u001b[1;32m    593\u001b[0m     mark_healthy\u001b[39m=\u001b[39;49mmark_healthy,\n\u001b[1;32m    594\u001b[0m )\n\u001b[1;32m    596\u001b[0m \u001b[39mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py:457\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m# Notice that we do not return the refs to any unfinished calls to the\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39m# user, since it is not safe to handle such remote actor calls outside the\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m# context of this actor manager. These requests are simply dropped.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m timeout \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(timeout_seconds) \u001b[39mif\u001b[39;00m timeout_seconds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m ready, _ \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    458\u001b[0m     remote_calls,\n\u001b[1;32m    459\u001b[0m     num_returns\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(remote_calls),\n\u001b[1;32m    460\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    461\u001b[0m     \u001b[39m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    462\u001b[0m     fetch_local\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m return_obj_refs,\n\u001b[1;32m    463\u001b[0m )\n\u001b[1;32m    465\u001b[0m \u001b[39m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m remote_results \u001b[39m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/ray/_private/worker.py:2578\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2576\u001b[0m timeout \u001b[39m=\u001b[39m timeout \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m6\u001b[39m\n\u001b[1;32m   2577\u001b[0m timeout_milliseconds \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[0;32m-> 2578\u001b[0m ready_ids, remaining_ids \u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39;49mcore_worker\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m   2579\u001b[0m     object_refs,\n\u001b[1;32m   2580\u001b[0m     num_returns,\n\u001b[1;32m   2581\u001b[0m     timeout_milliseconds,\n\u001b[1;32m   2582\u001b[0m     worker\u001b[39m.\u001b[39;49mcurrent_task_id,\n\u001b[1;32m   2583\u001b[0m     fetch_local,\n\u001b[1;32m   2584\u001b[0m )\n\u001b[1;32m   2585\u001b[0m \u001b[39mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1833\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:199\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "algo = config.build()  \n",
    "# algo.train() \n",
    "# Other settings we might adjust:\n",
    "# config[\"num_workers\"] = 10                       # Use > 1 for using more CPU cores, including over a cluster\n",
    "                                                # I.e., for each minibatch of data, do this many passes over it to train. \n",
    "# config[\"model\"][\"fcnet_hiddens\"] = [100, 50]    #\n",
    "\n",
    "# config.environment(disable_env_checking=True)\n",
    "\n",
    "# from ray.rllib.policy.policy import PolicySpec, Policy\n",
    "    \n",
    "# def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "#     return agent_id\n",
    "\n",
    "    \n",
    "\n",
    "# config[\"multiagent\"] = {\n",
    "#     \"policies\" : {\n",
    "#         \"0\" : PolicySpec(\n",
    "#             action_space=env.action_space\n",
    "#         ),\n",
    "#         \"1\" : PolicySpec(\n",
    "#             action_space=env.action_space\n",
    "#         )\n",
    "#     },\n",
    "#     \"policy_mapping_fn\":\n",
    "#             policy_mapping_fn,\n",
    "#     \"policies_to_train\": [\"0\", \"1\"],\n",
    "# }\n",
    "\n",
    "#Trainer\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = algo.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    if n%100 == 0:\n",
    "        file_name = algo.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')\n",
    "\n",
    "    with open('ma_ppo/rewards.json', 'w') as outfile:\n",
    "        json.dump(episode_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
