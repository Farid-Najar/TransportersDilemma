INFO - root :
Train PPO maskable started ! 

INFO - root :

        Environment information :
        
        Grid size = 15
        Q = 30
        K = 50
        n_vehicles = 4
         

INFO - root :
Number of CPUs = 8 

INFO - root :
the model parameters :
 {'policy_class': <class 'sb3_contrib.common.maskable.policies.MaskableActorCriticPolicy'>, 'device': device(type='cpu'), 'verbose': 1, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'share_features_extractor': True, 'net_arch': [2048, 2048, 1024, 256, 128]}, 'num_timesteps': 0, '_total_timesteps': 0, '_num_timesteps_at_start': 0, 'seed': None, 'action_noise': None, 'start_time': 0.0, 'learning_rate': 0.0003, 'tensorboard_log': '/Users/faridounet/PhD/TransportersDilemma/RL/ppo_mask/rewardMode(penalize_length)_steps(50000)/', '_last_obs': None, '_last_episode_starts': None, '_last_original_obs': None, '_episode_num': 0, 'use_sde': False, 'sde_sample_freq': -1, '_current_progress_remaining': 1.0, '_stats_window_size': 100, 'ep_info_buffer': None, 'ep_success_buffer': None, '_n_updates': 0, '_custom_logger': False, 'env': <stable_baselines3.common.vec_env.vec_monitor.VecMonitor object at 0x1637536a0>, '_vec_normalize_env': None, 'observation_space': Box(0.0, 10000000000.0, (283,), float64), 'action_space': Discrete(50), 'n_envs': 8, 'n_steps': 128, 'gamma': 0.99, 'gae_lambda': 0.95, 'ent_coef': 0.0, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'batch_size': 1024, 'n_epochs': 10, 'clip_range': <function constant_fn.<locals>.func at 0x1638f4790>, 'clip_range_vf': None, 'normalize_advantage': True, 'target_kl': None, 'lr_schedule': <function constant_fn.<locals>.func at 0x163876950>, 'policy': MaskableActorCriticPolicy(
  (features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (pi_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (vf_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential(
      (0): Linear(in_features=283, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=2048, bias=True)
      (3): ReLU()
      (4): Linear(in_features=2048, out_features=1024, bias=True)
      (5): ReLU()
      (6): Linear(in_features=1024, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=128, bias=True)
      (9): ReLU()
    )
    (value_net): Sequential(
      (0): Linear(in_features=283, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=2048, bias=True)
      (3): ReLU()
      (4): Linear(in_features=2048, out_features=1024, bias=True)
      (5): ReLU()
      (6): Linear(in_features=1024, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=128, bias=True)
      (9): ReLU()
    )
  )
  (action_net): Linear(in_features=128, out_features=50, bias=True)
  (value_net): Linear(in_features=128, out_features=1, bias=True)
), 'rollout_buffer': <sb3_contrib.common.maskable.buffers.MaskableRolloutBuffer object at 0x16388a1a0>} 

INFO - root :
Before training :
 mean, std = -38.20000076293945, 3.346640110015869 

INFO - root :
After training :
 mean, std = -38.5, 2.41867733001709 

