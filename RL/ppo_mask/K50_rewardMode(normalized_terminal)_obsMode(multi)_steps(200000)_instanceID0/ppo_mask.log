INFO - root :
Train PPO maskable started ! 

INFO - root :
Number of CPUs = 8 

INFO - root :
the model parameters :
 {'policy_class': <class 'sb3_contrib.common.maskable.policies.MaskableMultiInputActorCriticPolicy'>, 'device': device(type='cpu'), 'verbose': 1, 'policy_kwargs': {'features_extractor_class': <class '__main__.Multi'>}, 'num_timesteps': 0, '_total_timesteps': 0, '_num_timesteps_at_start': 0, 'seed': None, 'action_noise': None, 'start_time': 0.0, 'learning_rate': 0.0003, 'tensorboard_log': '/Users/faridounet/PhD/TransportersDilemma/RL/ppo_mask/K50_rewardMode(normalized_terminal)_obsMode(multi)_steps(200000)_instanceID0/', '_last_obs': None, '_last_episode_starts': None, '_last_original_obs': None, '_episode_num': 0, 'use_sde': False, 'sde_sample_freq': -1, '_current_progress_remaining': 1.0, '_stats_window_size': 100, 'ep_info_buffer': None, 'ep_success_buffer': None, '_n_updates': 0, '_custom_logger': False, 'env': <stable_baselines3.common.vec_env.vec_monitor.VecMonitor object at 0x103a928c0>, '_vec_normalize_env': None, 'observation_space': Dict('costs': Box(0.0, 1.0, (1, 51, 51), float64), 'other': Box(0.0, 1.0, (69,), float64)), 'action_space': Discrete(50), 'n_envs': 8, 'n_steps': 256, 'gamma': 0.99, 'gae_lambda': 0.95, 'ent_coef': 0.0, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'rollout_buffer_class': None, 'rollout_buffer_kwargs': {}, 'batch_size': 2048, 'n_epochs': 10, 'clip_range': <function constant_fn.<locals>.func at 0x17f12dfc0>, 'clip_range_vf': None, 'normalize_advantage': True, 'target_kl': None, 'lr_schedule': <function constant_fn.<locals>.func at 0x17f12dea0>, 'policy': MaskableMultiInputActorCriticPolicy(
  (features_extractor): Multi(
    (cnn): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=2670, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=2048, bias=True)
      (3): ReLU()
      (4): Linear(in_features=2048, out_features=1024, bias=True)
      (5): ReLU()
      (6): Linear(in_features=1024, out_features=512, bias=True)
      (7): ReLU()
    )
  )
  (pi_features_extractor): Multi(
    (cnn): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=2670, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=2048, bias=True)
      (3): ReLU()
      (4): Linear(in_features=2048, out_features=1024, bias=True)
      (5): ReLU()
      (6): Linear(in_features=1024, out_features=512, bias=True)
      (7): ReLU()
    )
  )
  (vf_features_extractor): Multi(
    (cnn): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=2670, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=2048, bias=True)
      (3): ReLU()
      (4): Linear(in_features=2048, out_features=1024, bias=True)
      (5): ReLU()
      (6): Linear(in_features=1024, out_features=512, bias=True)
      (7): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): Tanh()
    )
    (value_net): Sequential(
      (0): Linear(in_features=512, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): Tanh()
    )
  )
  (action_net): Linear(in_features=64, out_features=50, bias=True)
  (value_net): Linear(in_features=64, out_features=1, bias=True)
), 'rollout_buffer': <sb3_contrib.common.maskable.buffers.MaskableDictRolloutBuffer object at 0x17f11a620>} 

INFO - root :
Before training :
 mean, std = 0.31757575273513794, 0.0 

INFO - root :
After training :
 mean, std = 0.7890909910202026, 5.960464477539063e-08 

