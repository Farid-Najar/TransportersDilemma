INFO - root :
Train PPO maskable started ! 

INFO - root :

            Environment information :

            Grid size = 15
            Q = 35
            K = 100
            n_vehicles = 4
             

INFO - root :
Number of CPUs = 8 

INFO - root :
the model parameters :
 {'policy_class': <class 'sb3_contrib.common.maskable.policies.MaskableActorCriticPolicy'>, 'device': device(type='cpu'), 'verbose': 1, 'policy_kwargs': {'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'share_features_extractor': True, 'net_arch': [2048, 2048, 1024, 512]}, 'num_timesteps': 0, '_total_timesteps': 0, '_num_timesteps_at_start': 0, 'seed': None, 'action_noise': None, 'start_time': 0.0, 'learning_rate': 0.0003, 'tensorboard_log': '/Users/faridounet/PhD/TransportersDilemma/RL/ppo_mask/K100_rewardMode(normalized_terminal)_obsMode(routes)_steps(200000)_instanceID0/', '_last_obs': None, '_last_episode_starts': None, '_last_original_obs': None, '_episode_num': 0, 'use_sde': False, 'sde_sample_freq': -1, '_current_progress_remaining': 1.0, '_stats_window_size': 100, 'ep_info_buffer': None, 'ep_success_buffer': None, '_n_updates': 0, '_custom_logger': False, 'env': <stable_baselines3.common.vec_env.vec_monitor.VecMonitor object at 0x105cdfe80>, '_vec_normalize_env': None, 'observation_space': Box(0.0, 1.0, (413,), float64), 'action_space': Discrete(100), 'n_envs': 8, 'n_steps': 256, 'gamma': 0.99, 'gae_lambda': 0.95, 'ent_coef': 0.0, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'batch_size': 2048, 'n_epochs': 10, 'clip_range': <function constant_fn.<locals>.func at 0x16ff624d0>, 'clip_range_vf': None, 'normalize_advantage': True, 'target_kl': None, 'lr_schedule': <function constant_fn.<locals>.func at 0x16ff62440>, 'policy': MaskableActorCriticPolicy(
  (features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (pi_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (vf_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential(
      (0): Linear(in_features=413, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=2048, bias=True)
      (3): ReLU()
      (4): Linear(in_features=2048, out_features=1024, bias=True)
      (5): ReLU()
      (6): Linear(in_features=1024, out_features=512, bias=True)
      (7): ReLU()
    )
    (value_net): Sequential(
      (0): Linear(in_features=413, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=2048, bias=True)
      (3): ReLU()
      (4): Linear(in_features=2048, out_features=1024, bias=True)
      (5): ReLU()
      (6): Linear(in_features=1024, out_features=512, bias=True)
      (7): ReLU()
    )
  )
  (action_net): Linear(in_features=512, out_features=100, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
), 'rollout_buffer': <sb3_contrib.common.maskable.buffers.MaskableRolloutBuffer object at 0x17c0149d0>} 

INFO - root :
Before training :
 mean, std = 0.13671648502349854, 0.004305930808186531 

INFO - root :
After training :
 mean, std = 0.64362633228302, 5.960464477539063e-08 

